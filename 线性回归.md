# 线性回归

标签（空格分隔）： 有监督学习 回归

---
#### 主要思想

线性回归是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。

#### 产生背景

The earliest form of regression was the method of least squares, which was published by Legendre in 1805, and by Gauss in 1809.

#### 应用场景

* 预测趋势
    * 一条趋势线代表着时间序列数据的长期走势。它告诉我们一组特定数据（如GDP、石油价格和股票价格）是否在一段时期内增长或下降。 
* 线性回归分析可以用来量化相关性的强度？？

#### 核心理解

使用一条直线去拟合数据点？？

#### 主要推导

最小化平方误差
$$L(w,b) = \sum\limits_{i = 1}^N {{{(w{x_i} + b - {y_i})}^2}} $$
此函数为凸函数，求偏导为零可求的最优解w，b
$$\eqalign{
  & {{\partial L(w,b)} \over {\partial w}} = \sum\limits_{i = 1}^N {2(w{x_i} + b - {y_i}){x_i}}   \cr 
  & {{\partial L(w,b)} \over {\partial b}} = \sum\limits_{i = 1}^N {2(w{x_i} + b - {y_i})}  \cr} $$

同时此方法可以写成代数形式求解

#### 求解算法

最小二乘法，特征数大于样本数，可能得到多组解。

#### 伪代码

```

输入:数据集D(N个样本)

过程:

通过使用推导中的公式计算w和b，可以使用代数形式求解，此时矩阵求逆改为求解方程组的方法求。
    
输出:线性回归方程

```

#### 复杂度分析

时间复杂度：非代数形式O(N),代数形式[代码及复杂度][1]

#### 大数据下改进

to be done

#### 评价

* 优点

  * 简单

* 缺点

  * 可能过拟合，可能有多组解
#### 算法改进

* Lasso回归(加入L1正则)
* Ridge回归(加入L2正则)
* Elastic Net(加入L1和L2正则)

#### 参考资料

1. [wiki](https://en.wikipedia.org/wiki/Linear_regression#Extensions)

2. [scikit-learn](http://scikit-learn.org/stable/modules/linear_model.html)


  [1]: https://github.com/numpy/numpy/blob/v1.11.0/numpy/linalg/linalg.py#L1785-L1943