# 神经网络(多层前馈神经网络)

标签（空格分隔）： 有监督学习 分类 回归

---

#### 主要思想

通过仿生学模拟生物体内神经元的构造，神经网络由大量的人工神经元联结进行计算。

#### 产生背景

Warren McCulloch and Walter Pitts (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. 

#### 应用场景

* 分类
* 回归？？

#### 核心理解

每一层都是判断输入向量和当前层的权重向量是否match的过程？？pattern extraction with layers of connection weights[林轩田课程]

#### 主要推导(主要引自周志华-机器学习)

![此处输入图片的描述][1]
上图给出了一个拥有d个输入神经元，l个输出神经元，q个隐层神经元的多层前馈神经网络。其中输出层第j个神经元的阈值用${\theta _j}$表示，隐层第h个神经元的阈值用${\gamma _h}$表示。
假设隐层和输出层都采用sigmoid函数做激活函数。

对于训练样例$({x_k},{y_k})$，假定神经网络的输出为${{\hat y}_k} = \{ \hat y_1^k,\hat y_2^k...\hat y_l^k\} $，此处
$$\hat y_j^k = f({\beta _j} - {\theta _j})$$
此神经网络在样例$({x_k},{y_k})$上的均方误差为：
$${E_k} = \sum\limits_{j = 1}^l {{{(\hat y_j^k - y_j^k)}^2}} $$

我们需要通过训练得到的参数有：
* 输入层到隐含层的d*q个权值
* 隐含层到输出层的q*l个权值
* q个隐层神经元的阈值
* l个输出层神经元的阈值

BP是一个迭代学习算法，在迭代的每一轮采用广义的感知机学习规则对参数进行更新估计，如何更新我们使用${w_{hj}}$为例进行推导。

BP使用基于梯度下降策略，以目标负梯度方向对参数进行调整。对误差${E_k}$，给定学习率$\eta $，有

$$\Delta {w_{hj}} =  - \eta {{\partial {E_k}} \over {\partial {w_{hj}}}}$$

注意到${{w_{hj}}}$先影响到第j个输出层神经元的输入值${\beta _j}$，再影响到其输出值$\hat y_j^k$，然后影响到${E_k}$，有

$${{\partial {E_k}} \over {\partial {w_{hj}}}} = {{\partial {E_k}} \over {\partial \hat y_j^k}} \cdot {{\partial \hat y_j^k} \over {\partial {\beta _j}}} \cdot {{\partial {\beta _j}} \over {\partial {w_{hj}}}}$$

根据${\beta _j}$的定义，显然有
$${{\partial {\beta _j}} \over {\partial {w_{hj}}}} = {b_h}$$
sigmoid函数有一个很好的性质
$$f'(x) = f(x)(1 - f(x))$$

我们设
$$\eqalign{
  & {g_j} =  - {{\partial {E_k}} \over {\partial \hat y_j^k}} \cdot {{\partial \hat y_j^k} \over {\partial {\beta _j}}}  \cr 
  &  =  - (\hat y_j^k - y_j^k)f'({\beta _j} - {\theta _j})  \cr 
  &  = \hat y_j^k(1 - \hat y_j^k)(y_j^k - \hat y_j^k) \cr} $$
  
通过上式可得

$$\Delta {w_{hj}} = \eta {g_j}{b_h}$$

类似可得

$$\eqalign{
  & \Delta {\theta _j} =  - \eta {g_j}  \cr 
  & \Delta {v_{ih}} = \eta {e_h}{x_i}  \cr 
  & \Delta {\gamma _h} =  - \eta {e_h} \cr} $$

其中

$$\eqalign{
  & {e_h} = {{\partial {E_k}} \over {\partial {b_h}}} \cdot {{\partial {b_h}} \over {\partial {\alpha _h}}}  \cr 
  &  =  - \sum\limits_{j = 1}^l {{{\partial {E_k}} \over {\partial {\beta _j}}} \cdot {{\partial {b_h}} \over {\partial {\alpha _h}}}} f'({\alpha _h} - {\gamma _h})  \cr 
  &  =  - \sum\limits_{j = 1}^l {{w_{hj}} \cdot {g_j}} f'({\alpha _h} - {\gamma _h})  \cr 
  &  = {b_h}(1 - {b_h})\sum\limits_{j = 1}^l {{w_{hj}} \cdot {g_j}}  \cr} $$
  

#### 求解算法

BP算法，对于每个训练样例，BP算法执行以下操作
1.先将输入示例提供给输入层神经元，然后逐层将信号前传，直到产生输出层的结果。
2.然后计算输出层的误差，再将误差逆向传播至隐层神经元。
3.最后根据隐层神经元的误差来对连接权和阈值进行调整。
该迭代过程循环进行，知道达到某些停止条件为止。

#### 伪代码

```

输入:数据集D(N个样本)，学习率

过程:在(0,1)范围内随机初始化网络中所有连接权和阈值

repeat
    for all 数据集中的样本 do
    根据当前参数计算当前样本的输出
    计算输出层神经元的梯度项
    计算隐层神经元的梯度项
    更新所有连接权和阈值
end for

until:达到停止条件

输出:连接权和阈值确定的多层前馈神经网络

```

#### 大数据下改进

to be done

#### 评价

* 优点

  * 拟合能力强，至于一个包含足够多神经元的隐层，多层前馈网络就能以任意精度逼近任意复杂度的连续函数。

* 缺点

  * 容易过拟合
  
#### 算法改进

* 相应措施(early stopping/正则)防止过拟合
* 深度神经网络

#### 参考资料

1. 周志华 机器学习

  [1]: http://i.imgur.com/vFyC1vx.png
  [2]: https://github.com/numpy/numpy/blob/v1.11.0/numpy/linalg/linalg.py#L1785-L1943